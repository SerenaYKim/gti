# -*- coding: utf-8 -*-
"""Copy of 2025-08-14-GTI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tNqgnTm4Yqao7imqp_vtbCmS07VgclN9
"""

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

df = pd.read_csv("/content/drive/MyDrive/n20-workshops/gti-fulbright-2025/data.csv")

df

def add_sentiment(df, text_col="text", model_name="cardiffnlp/twitter-roberta-base-sentiment", batch_size=64):
    # Handle empty
    if df.empty:
        return df.copy()

    # Load model & tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    device = 0 if torch.cuda.is_available() else -1

    pipe = TextClassificationPipeline(
        model=model,
        tokenizer=tokenizer,
        task="text-classification",
        device=device,
        truncation=True,
        padding=True,
        max_length=256,
        return_all_scores=True
    )

    texts = df[text_col].fillna("").tolist()

    # Run in batches
    results = []
    for i in range(0, len(texts), batch_size):
        batch_out = pipe(texts[i:i+batch_size])
        results.extend(batch_out)

    # Map labels if using CardiffNLP
    label_map_cardiff = {"LABEL_0": "negative", "LABEL_1": "neutral", "LABEL_2": "positive"}
    use_cardiff_map = "cardiffnlp/twitter-roberta-base-sentiment" in model_name.lower()

    # Determine final label names (for prob columns)
    raw_labels = [d["label"] for d in results[0]]
    final_labels = []
    for lab in raw_labels:
        if use_cardiff_map:
            final_labels.append(label_map_cardiff.get(lab, lab).lower())
        else:
            final_labels.append(lab.lower())

    # Collect outputs
    top_labels = []
    top_scores = {}
    for lab in final_labels:
        top_scores[lab] = []

    per_row_top_score = []
    per_row_top_label = []

    for r in results:
        # pick top
        best = max(r, key=lambda x: x["score"])
        best_label_raw = best["label"]
        if use_cardiff_map:
            best_label = label_map_cardiff.get(best_label_raw, best_label_raw).lower()
        else:
            best_label = best_label_raw.lower()
        per_row_top_label.append(best_label)
        per_row_top_score.append(best["score"])

        # probs per class
        row_probs = {}
        for item in r:
            raw = item["label"]
            if use_cardiff_map:
                norm = label_map_cardiff.get(raw, raw).lower()
            else:
                norm = raw.lower()
            row_probs[norm] = item["score"]

        for lab in final_labels:
            top_scores[lab].append(row_probs.get(lab, 0.0))

    out = df.copy()
    out["sentiment"] = per_row_top_label
    out["sent_confidence"] = per_row_top_score
    for lab in final_labels:
        out["prob_" + lab] = top_scores[lab]

    return out

model_name = "cardiffnlp/twitter-roberta-base-sentiment"
# model_name = "oliverguhr/german-sentiment-bert"  # if your texts are German

df_with_sent_score = add_sentiment(df, text_col="text", model_name=model_name, batch_size=64)
df_sent = df_with_sent_score.copy()

df_sent

df_sent.to_csv("", index = False)

"""# Mapping"""

# ======================
# Part 1 — Blank GeoDataFrame
# ======================
import pandas as pd
import geopandas as gpd

city_coords = {
    "Berlin":      (52.5200, 13.4050),
    "Hamburg":     (53.5511,  9.9937),
    "Munich":      (48.1374, 11.5755),
    "Cologne":     (50.9375,  6.9603),
    "Frankfurt":   (50.1106,  8.6821),
    "Stuttgart":   (48.7784,  9.1806),
    "Dusseldorf":  (51.2277,  6.7735),
    "Leipzig":     (51.3397, 12.3731),
    "Dortmund":    (51.5136,  7.4653),
    "Essen":       (51.4556,  7.0116),
}

cities_df = pd.DataFrame(
    [{"city": c, "lat": lat, "lon": lon} for c, (lat, lon) in city_coords.items()]
)

gdf = gpd.GeoDataFrame(
    cities_df,
    geometry=gpd.points_from_xy(cities_df["lon"], cities_df["lat"]),
    crs="EPSG:4326"
)

import folium

m = folium.Map(location=[51.1657, 10.4515], zoom_start=6, tiles="CartoDB positron")
m

# ======================
# Part 2 — Compute tweet scores and merge with GeoDataFrame
# ======================
# Sentiment to numeric score mapping
sent_map = {"positive": 10, "neutral": 0, "negative": -10}

df_scored = df_sent.copy()
df_scored["sentiment"] = df_scored["sentiment"].str.lower()
df_scored["sent_score"] = df_scored["sentiment"].map(sent_map).fillna(0)

# Average score per city
city_avg = df_scored.groupby("city", as_index=False)["sent_score"].mean()

# Count number of tweets per sentiment type per city
city_counts = df_scored.groupby(["city", "sentiment"]).size().unstack(fill_value=0)

# Calculate proportions (%) for each sentiment
city_props = city_counts.div(city_counts.sum(axis=1), axis=0) * 100
city_props = city_props.reset_index()

# Merge average score and proportions
city_stats = pd.merge(city_avg, city_props, on="city", how="outer")

# Merge onto the GeoDataFrame (fill NaNs with 0 so all cities display)
gdf = gdf.merge(city_stats, on="city", how="left").fillna(0)

# ======================
# Part 3 — Generate the final map
# ======================
import branca.colormap as cm

colormap = cm.LinearColormap(
    colors=["#d7191c", "#fdae61", "#ffffbf", "#a6d96a", "#1a9641"],
    vmin=-10, vmax=10
)
colormap.caption = "Average Sentiment (−10 negative … +10 positive)"
colormap.add_to(m)

for _, row in gdf.iterrows():
    score = float(row["sent_score"])
    neg_pct = row.get("negative", 0)
    neu_pct = row.get("neutral", 0)
    pos_pct = row.get("positive", 0)

    tooltip_text = (
        f"{row['city']} Sentiment Score: {score:.2f}<br>"
        f"Negative: {neg_pct:.1f}%<br>"
        f"Neutral: {neu_pct:.1f}%<br>"
        f"Positive: {pos_pct:.1f}%"
    )

    folium.CircleMarker(
        location=[row["lat"], row["lon"]],
        radius=9,
        color=colormap(score),
        fill=True,
        fill_opacity=0.85,
        weight=1,
        tooltip=tooltip_text
    ).add_to(m)

m